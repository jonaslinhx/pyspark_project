[LOCAL]
spark.master = local[2]
spark.driver.extraJavaOptions = "-Dlog4j.configuration=file:log4j.properties"
spark.app.name = pyspark_project-local
spark.executor.instances = 2
spark.executor.core = 1
spark.executor.memory = 1G
spark.sql.shuffle.partitions = 5
emable.hive = false
hive.database = null
account.filter = active_ind = 1
party.filter = 
address.filter = 
kafka.topic = pyspark_project_kafka_cloud
kafka.bootstrap.srvers = pkc-4j8dq.southeastasia.azure.confluent.cloud:9092
kafka.client.dns.lookup = use_all_dns_ips
kafka.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}'
kafka.api_key = 45R6XQ67ENKF7JUM
kafka.api_secret = sdfsf
kafka.security.protocol = 
kafka.sasl.mechanism = 

[QA]
spark.app.name = pyspark_project-qa
spark.executor.instances = 2
spark.executor.core = 1
spark.executor.memory = 4G
spark.sql.shuffle.partitions = 1000
[PROD]
spark.app.name = pyspark_project
spark.executor.instances = 2
spark.executor.core = 1
spark.executor.memory = 4G
spark.sql.shuffle.partitions = 1000